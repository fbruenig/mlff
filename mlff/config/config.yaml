workdir: first_experiment  # Working directory. Checkpoints and hyperparameters are saved there.
data:
  filepath: null  # Path to the data file. Either ASE digestible or .npz with appropriate column names are supported.
  energy_unit: eV  # Energy unit.
  length_unit: Angstrom  # Length unit.
  energy_shifts: null  # Energy shifts to subtract.
  split_seed: 0  # Seed using for splitting the data into training, validation and test.
model:
  num_layers: 2  # Number of message passing layers.
  num_features: 128  # Number of invariant features.
  num_heads: 4  # Number of heads on the invariant features.
  num_features_head: 32  # Number of features per head used for the invariant features and the Euclidean variables.
  degrees:  # The degrees of spherical harmonics to use in the Euclidean variables.
    - 1
    - 2
    - 3
    - 4
  cutoff: 5.0  # Local cutoff to use.
  cutoff_fn: exponential  # Cutoff function to use.
  num_radial_basis_fn: 16  # Number of radial basis functions.
  radial_basis_fn: bernstein  # Radial basis function to use.
  activation_fn: silu  # Activation function used in the MLPs.
  qk_non_linearity: silu  # Non-linearity to apply to the query and key vectors in the attention function.
  residual_mlp_1: true  # Use residual MLP on the invariant features after the the attention update.
  residual_mlp_2: true  # Use residual MLP on the invariant features after the the exchange block.
  layer_normalization_1: false  # Use layer normalization after first residual mlp.
  layer_normalization_2: false  # Use layer normalization after the second residual mlp.
  layers_behave_like_identity_fn_at_init: false  # The message passing layers behave like the identity function at initialization.
  output_is_zero_at_init: true  # The output of the full network is zero at initialization.
  input_convention: positions  # Input convention.
optimizer:
  name: adam  # Name of the optimizer. See https://optax.readthedocs.io/en/latest/api.html#common-optimizers for available ones.
  learning_rate: 0.001  # Learning rate to use.
  learning_rate_schedule: exponential_decay  # Which learning rate schedule to use. See https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules for available ones.
  learning_rate_schedule_args:  # Arguments passed to the learning rate schedule. See https://optax.readthedocs.io/en/latest/api.html#optimizer-schedules.
    decay_rate: 0.75
    transition_steps: 125000
  num_of_nans_to_ignore: 0  # Number of repeated update/gradient steps that ignore NaNs before raising on error.
training:
  allow_restart: false  # Re-starting from checkpoint is allowed. This will overwrite existing checkpoints so only use if this is desired.
  num_epochs: 100  # Number of epochs.
  num_train: 950  # Number of training points to draw from data.filepath.
  num_valid: 50  # Number of validation points to draw from data.filepath.
  batch_max_num_nodes: null  # Maximal number of nodes per batch. Must be at least maximal number of atoms + 1 in the data set.
  batch_max_num_edges: null  # Maximal number of edges per batch. Must be at least maximal number of edges + 1 in the data set.
  # If batch_max_num_nodes and batch_max_num_edges is set to null, they will be determined from the max_num_of_graphs.
  # If they are set to values, each batch will contain as many molecular structures/graphs such none of the three values
  # batch_max_num_nodes, batch_max_num_edges and batch_max_num_of_graphs is exceeded.
  batch_max_num_graphs: 6  # Maximal number of graphs per batch.
  # Since there is one padding graph involved for an effective batch size of 5 corresponds to 6 max_num_graphs.
  eval_every_num_steps: 1000  # Number of gradient steps after which the metrics on the validation set are calculated.
  loss_weights:
    energy: 0.01  # Loss weight for the energy.
    forces: 0.99  # Loss weight for the forces.
  model_seed: 0  # Seed used for the initialization of the model parameters.
  training_seed: 0  # Seed used for shuffling the batches during training.
  wandb_init_args:  # Arguments to wandb.init(). See https://docs.wandb.ai/ref/python/init. The config itself is passed as config to wandb.init().
    name: first_training_run
    project: mlff
    group: null
