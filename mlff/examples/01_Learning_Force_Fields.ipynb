{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "from mlff.src.data.preprocessing import split_data\n",
    "from mlff.src.training import Optimizer, Coach, get_loss_fn, create_train_state\n",
    "from mlff.src.io.io import create_directory, merge_dicts, bundle_dicts, save_dict\n",
    "from mlff.src.data import DataTuple, DataSet\n",
    "from mlff.src.indexing.indices import get_indices\n",
    "from mlff.src.nn.stacknet import StackNet, get_obs_and_grad_obs_fn, get_obs_and_force_fn\n",
    "from mlff.src.nn.embed import AtomTypeEmbed, GeometryEmbed\n",
    "from mlff.src.nn.layer import So3kratesLayer\n",
    "from mlff.src.nn.observable import Energy\n",
    "import wandb\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by giving the path to the data we want to train our model on, as well as the path where we want \n",
    "# to save the model checkpoints as well as the hyperparamter file. \n",
    "\n",
    "data_path = 'example_data/ethanol.npz'\n",
    "save_path = 'example_model/'\n",
    "ckpt_dir = os.path.join(save_path, 'module')\n",
    "ckpt_dir = create_directory(ckpt_dir, exists_ok=False)\n",
    "\n",
    "# next we define the property keys. The keys in the dictionary should not be changed, but the values can be \n",
    "# changed arbitrarily such that they match the names in the data file.\n",
    "\n",
    "prop_keys = {'energy': 'E',\n",
    "             'force': 'F',\n",
    "             'atomic_type': 'z',\n",
    "             'atomic_position': 'R',\n",
    "             'hirshfeld_volume': None,\n",
    "             'total_charge': None,\n",
    "             'total_spin': None,\n",
    "             'partial_charge': None\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now load the data. We assume that the data is given or has been transformed to .npz format.\n",
    "data = dict(np.load(data_path))\n",
    "\n",
    "# Initialize a DataSet object with the property keys and the loaded (and dict transformed) data set. For \n",
    "# a more detailed introduction into the DataSet object take a look at the 00_Data_Preparation.ipynb example.\n",
    "md17_dataset = DataSet(prop_keys=prop_keys, data=data)\n",
    "\n",
    "# Since StackNets work on neighborhood index lists which often are not part of the input data, the split function\n",
    "# takes an argument for the cutoff radius as input. Based on the cutoff radius neighborhood lists are \n",
    "# constructed which are saved as 'idx_i' and 'idx_j' in the returned dictionary. The former is the centering\n",
    "# atom and the latter the neighboring atoms.\n",
    "r_cut = 5.\n",
    "# Next, we split the data, where we define the keys that should be split into training, validation and testing\n",
    "# data.\n",
    "d = md17_dataset.random_split(n_train=100,\n",
    "                              n_valid=100,\n",
    "                              n_test=None,\n",
    "                              training=True,\n",
    "                              seed=0,\n",
    "                              r_cut=r_cut)\n",
    "\n",
    "# The resulting dictionary, has mutliple keys which also includes keys that have not been defined as quantities to\n",
    "# split. For that reason they are in the upper level of the dictionary.\n",
    "print(list(d.keys()))\n",
    "\n",
    "md17_dataset.save_splits_to_file(path=ckpt_dir, filename='my_first_training_split.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also see that there are the keys called 'train', 'valid' and 'test', respectively. Each key contains\n",
    "# another dictionary which contains the splitted quantities as numpy arrays: E.g.\n",
    "print('Keys in the training split: {}'.format(list(d['train'].keys())))\n",
    "print('Shape of the atomic positions in the training set: {}'.format(d['train']['R'].shape))\n",
    "print('Shape of the atomic positions in the test set: {}'.format(d['test']['R'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each StackNet consists of 4 building blocks.\n",
    "\n",
    "# 1) Sequence of modules that embed the geometry of molecule. Here we choose a single module, that returns geometry \n",
    "# related quantities such that the expansion of the interatomic distance vectors in spherical harmonics, the \n",
    "# expansion of the interatomic distances in some radial basis function as well as cutoff function related \n",
    "# quantities. It also takes as input the prop_keys dictionary in order to \"know\" the name of \n",
    "# the atomic positions in the data.\n",
    "geometry_embeddings = [GeometryEmbed(degrees=[1, 2],\n",
    "                                    radial_basis_function='phys',\n",
    "                                    n_rbf=32,\n",
    "                                    radial_cutoff_fn='cosine_cutoff_fn',\n",
    "                                    r_cut=r_cut,\n",
    "                                    prop_keys=prop_keys,\n",
    "                                    sphc=True\n",
    "                                    )]\n",
    "\n",
    "# 2) A list of modules that embed different input quantities. Since in our example we only have atomic types\n",
    "# as input, we only use the `AtomTypeEmbed` module. It takes the atomic embeddings and returns a feature vectors\n",
    "# of dimension `features` based on the atomic type.\n",
    "embeddings = [AtomTypeEmbed(num_embeddings=100, features=32, prop_keys=prop_keys)]\n",
    "\n",
    "\n",
    "# 3) A list of modules that represent layers. Here we use 2 So3krates layers.\n",
    "so3krates_layer = [So3kratesLayer(fb_filter='radial_spherical',\n",
    "                                  fb_rad_filter_features=[32, 32],\n",
    "                                  fb_sph_filter_features=[32, 32],\n",
    "                                  fb_attention='conv_att',\n",
    "                                  gb_filter='radial_spherical',\n",
    "                                  gb_rad_filter_features=[32, 32],\n",
    "                                  gb_sph_filter_features=[32, 32],\n",
    "                                  gb_attention='conv_att',\n",
    "                                  degrees=[1, 2],\n",
    "                                  n_heads=2,\n",
    "                                  chi_cut=None,\n",
    "                                  ) for _ in range(2)]\n",
    "\n",
    "\n",
    "# 4) A list of observable modules that are not related to the input by some differential operator. E.g. forces are \n",
    "# the gradient wrt the energy, thus it will be defined as an extra observable in the next step. We additionally\n",
    "# rescale the energy output of the network using a per atom scale and a per atom shift. Here we choose the mean\n",
    "# over all training energies divided by the number of atoms as per atom shift and the standard deviation of all \n",
    "# force components as scale. Note that one can also rescale the target data instead of the energy output. However,\n",
    "# by making the scaling a quantity of the network itself, it can be applied later without any reference to the \n",
    "# training data. However, the learning rate used has to be scaled accordingly, since the loss will be larger\n",
    "# in the setting of rescaling the network output.\n",
    "F_scale = d['train']['F_scale']\n",
    "E_mean = d['train']['E_mean']\n",
    "n_atoms = d['train']['z'].shape[-1]\n",
    "obs = [Energy(per_atom_scale=[F_scale.tolist()]*20, per_atom_shift=[(E_mean/n_atoms).tolist()]*20, prop_keys=prop_keys)]\n",
    "\n",
    "\n",
    "# We now put everything together into the StackNet.\n",
    "net = StackNet(geometry_embeddings=geometry_embeddings,\n",
    "               feature_embeddings=embeddings,\n",
    "               layers=so3krates_layer,\n",
    "               observables=obs,\n",
    "               prop_keys=prop_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All quantities that can be written D_{input}(output) where D_{inputs} denotes a differential operator wrt\n",
    "# to some input quantity can be defined as additional observable after the network has been initialized.\n",
    "# For force fields, this additional quantitie usually is the force, which is the negative gradient of the energy wrt\n",
    "# the atomic positions. \n",
    "# Derivatives are defined as a `Tuple[name: str, Tuple[output_key: str, input_key: str, transform: Callable]]`, \n",
    "# where `name` determines the key in the final observable dictionary (output of `obs_fn`). The second tuple has \n",
    "# in total three entries where the first is the key of the output quantity and the second the input quantity that\n",
    "# wrt the derivative should be taken. The third entry is a transformation which can be used the scale (here the \n",
    "# negtive sign) and perform further operations on the output. Since the energy output has shape (1) the gradient\n",
    "# wrt the positions has shape (1,n_atoms,3) which is why we squeeze away the 0-th dimension such that forces \n",
    "# have shape (n_atoms,3)\n",
    "\n",
    "D_force = (prop_keys['force'], (prop_keys['energy'], prop_keys['atomic_position'], lambda y: -y.squeeze(-3)))\n",
    "obs_fn = get_obs_and_grad_obs_fn(net, derivatives=(D_force, ))\n",
    "\n",
    "# Alternatively, as forces are often the only gradient observable, mlff also provides a pre-implemented \n",
    "# function that returns an observable function for all observables in the StackNet as well as the forces.\n",
    "# It could be used instead of `obs_fn` for all following steps.\n",
    "obs_fn_ = get_obs_and_force_fn(net)\n",
    "\n",
    "# Since all code internally assumes no batch dimension, we vmap the input over the batch dimension. We will \n",
    "obs_fn = jax.vmap(obs_fn, in_axes=(None, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pformat\n",
    "\n",
    "n_data = 2\n",
    "init_input = {k:jnp.array(v[0]) for (k, v) in d['train'].items() if k in \n",
    "              [prop_keys['atomic_position'], prop_keys['atomic_type'], 'idx_i', 'idx_j']}\n",
    "params = net.init(jax.random.PRNGKey(1234), init_input)\n",
    "fwd_input = {k:v[:n_data] for (k, v) in d['train'].items() if k in \n",
    "             [prop_keys['atomic_position'], prop_keys['atomic_type'], 'idx_i', 'idx_j']}\n",
    "\n",
    "# Lets see how the output of the obs_fn looks like. It returns a dictionary where each observable has \n",
    "# its own entry that can be accessed using the corresponding key of the observable that is specified in the \n",
    "# props_key dictionay in the very beginning.\n",
    "\n",
    "observables = obs_fn(params, fwd_input)\n",
    "print('All observables:\\n {}'.format(pformat(observables)))\n",
    "print('Energy:\\n {}'.format(pformat(observables[prop_keys['energy']])))\n",
    "print('Force:\\n {}'.format(pformat(observables[prop_keys['force']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlff provides a default optimizer that follows the default settings for the AdamW optimizer. It can be\n",
    "# initialized by just initializing the optimizer class. An optax optimizer will be returned by calling the\n",
    "# `.get()` method, which takes a learning rate and returns the corresponding optax optimizer. Note, that \n",
    "# if we are using a network that scales and shifts its output, one has to use larger learning rates\n",
    "# as if one is training on rescaled training data. This is due to the fact, that the variance of the targets\n",
    "# (and thus the gradients wrt to the loss function) is larger if the training data is not rescaled. Thus, \n",
    "# usual learning rates have to be increased by the same order of magnitude. If not rescaling the data itself,\n",
    "# we found training to be more stable when transforming kcal/mol to eV.\n",
    "opt = Optimizer()\n",
    "tx = opt.get(learning_rate=1e-3*np.sqrt(F_scale))\n",
    "\n",
    "\n",
    "# mlff also provied a Coach `dataclass` which is used as storage for all quantities that are associated \n",
    "# with the training process. \n",
    "# The `input_keys` determine which quantities shall be used during the training process.\n",
    "# Here one has to make sure, that the input data provides all neccessary quantitities that are used by \n",
    "# the network. E.g. if one is using the `AtomTypeEmbed` module, one has to make sure that the atomic types\n",
    "# are passed as an input.\n",
    "# The `target_keys` determine which observables enter the calculcation of the loss function. Here we \n",
    "# chose energy and forces. The network can still output additional observables, e.g. the partial charges\n",
    "# which are just not taken into account for the calculcation of the loss if not listed here. This can be\n",
    "# useful if one want to use the same model for different training routines where one starts by training\n",
    "# only a subset of the quantities. However, the observable function must output all observables that are \n",
    "# listed here. \n",
    "# The `loss_weight` attribute, assigns the scaling parameter to different quantities used in the loss function.\n",
    "coach = Coach(input_keys=[prop_keys['atomic_position'], prop_keys['atomic_type'], 'idx_i', 'idx_j'],\n",
    "              target_keys=[prop_keys['energy'], prop_keys['force']],\n",
    "              epochs=200,\n",
    "              training_batch_size=2,\n",
    "              validation_batch_size=2,\n",
    "              loss_weights={prop_keys['energy']: .01, prop_keys['force']: .99},\n",
    "              ckpt_dir=ckpt_dir,\n",
    "              data_path=data_path,\n",
    "              net_seed=0,\n",
    "              training_seed=0)\n",
    "\n",
    "# The `get_loss_fn` method, returns a loss function given the observable function and the `loss_weights`.\n",
    "# As the loss function is acessing the `loss_weights` during training, the specific loss function only \n",
    "# works for the given observables.\n",
    "loss_fn = get_loss_fn(obs_fn=obs_fn, weights=coach.loss_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before starting the training, we have to split the training and validation data into input and output \n",
    "# quantitites. This is done using the `DataTuple` class, which is initialized using `input_keys` and \n",
    "# `target_keys` which are just lists keys.\n",
    "\n",
    "data_tuple = DataTuple(input_keys=coach.input_keys,\n",
    "                       target_keys=coach.target_keys)\n",
    "\n",
    "# After initializing the `DataTuple` we can call it on the data which has been splitted using the methods from\n",
    "# before. `train_ds` and `valid_ds` are Tuples[Array, Array] where the first entry is the inputs and the second\n",
    "# the target data.\n",
    "train_ds = data_tuple(d['train'])\n",
    "valid_ds = data_tuple(d['valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Inputs:\\n {}'.format(pformat(jax.tree_map(lambda y: y[:2], train_ds[0]))))\n",
    "print('Outputs:\\n {}'.format(pformat(jax.tree_map(lambda y: y[:2], train_ds[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At last step we have to initialize the parameters of the network. As the init function can not be vmaped\n",
    "# and internally we assume no batch dimension, ons has to initialize with data that has no batch dimension.\n",
    "# Here this is achieved using the `jax.tree_map` function which selects a single data point for all input\n",
    "# quantities. After we have initialized the parameters, the `net`, its `params` and the optax optimizer `tx`\n",
    "# are used to create the `train_state` which handles the gradient updates and checkpoints. The method further\n",
    "# returns a dictionary representation for the train_state hyperparameters.\n",
    "# The parameters are a `FrozenDict` and contains all initialized parameters of the network.\n",
    "inputs = jax.tree_map(lambda x: jnp.array(x[0, ...]), train_ds[0])\n",
    "params = net.init(jax.random.PRNGKey(coach.net_seed), inputs)\n",
    "train_state, h_train_state = create_train_state(net, \n",
    "                                                params, \n",
    "                                                tx, \n",
    "                                                scheduled_lr_decay={'exponential': {'transition_steps': 50_000,\n",
    "                                                                                    'decay_factor': 0.5}\n",
    "                                                                   }\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to reproduce and save all information, all classes in mlff implement a `__dict_repr__()` method, \n",
    "# that returns a dictionary representation of the class. This can be used to e.g. load the model after training\n",
    "# to use it for evaluation. We show how to use a trained model below.\n",
    "h_net = net.__dict_repr__()\n",
    "h_opt = opt.__dict_repr__()\n",
    "h_coach = coach.__dict_repr__()\n",
    "h_dataset = md17_dataset.__dict_repr__()\n",
    "h = bundle_dicts([h_net, h_opt, h_coach, h_dataset, h_train_state])\n",
    "save_dict(path=ckpt_dir, filename='hyperparameters.json', data=h, exists_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialize the weight and bias project. For all possible parameters passed to the .init() method check\n",
    "# https://docs.wandb.ai/ref/python/init\n",
    "wandb.init(project='mlff', name='my_first_force_field', config=h)  \n",
    "# We can use the `.run()` method of the `Coach` class to run training.\n",
    "# Note, that the first call might take some time, since JAX compiles the computational graph\n",
    "# for heavy optimization and parallelization. \n",
    "coach.run(train_state=train_state, \n",
    "          train_ds=train_ds, \n",
    "          valid_ds=valid_ds, \n",
    "          loss_fn=loss_fn,\n",
    "          log_every_t=1,\n",
    "          eval_every_t=100,  # evaluate validation loss every t gradient steps\n",
    "          ckpt_overwrite=False)\n",
    "# after running the training once, see what changes if you try to run the training again with \n",
    "# `ckpt_overwrite=False` and `ckpt_overwrite=True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on Same Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlff.src.io.io import read_json\n",
    "from mlff.src.nn.stacknet import init_stack_net\n",
    "from flax.training import checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the hyperparemter file and restore the coach and initialize the StackNet. Since Coach is a simple \n",
    "# dataclass it can be directly loaded using the dictionary. For the StackNet, one needs to use the function\n",
    "# `init_stack_net()` which initializes the underlying modules in the StackNet given the hyperparameters.\n",
    "h = read_json(os.path.join(ckpt_dir, 'hyperparameters.json'))\n",
    "\n",
    "coach = Coach(**h['coach'])\n",
    "test_data_tuple = DataTuple(input_keys=coach.input_keys,\n",
    "                            target_keys=coach.target_keys)\n",
    "\n",
    "test_net = init_stack_net(h)\n",
    "\n",
    "test_data = dict(np.load(coach.data_path))\n",
    "test_dataset = DataSet(data=test_data, prop_keys=prop_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = test_dataset.load_split(file=os.path.join(ckpt_dir, 'my_first_training_split.json'), \n",
    "                                 r_cut=5.,\n",
    "                                 n_test=200,\n",
    "                                 split_name='random_split')\n",
    "test_input, test_obs = test_data_tuple(d_test['test'])\n",
    "# In total 400 geometries are used, since training and validation data are also restored. If you want to turn this \n",
    "# off, set n_train and n_test to 0 in the above function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_obs_fn = get_obs_and_force_fn(test_net)\n",
    "test_obs_fn = jax.jit(jax.vmap(test_obs_fn, in_axes=(None, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the trained parameters from the checkpoint directory. For the behavior of `.restore_checkpoint()` method\n",
    "# check the FLAX github or documentation.\n",
    "test_params = checkpoints.restore_checkpoint(ckpt_dir=ckpt_dir, target=None)['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict energy and forces and calculate the mean absolute errors.\n",
    "obs_pred = test_obs_fn(test_params, test_input)\n",
    "e_mae = np.abs(obs_pred['E'].reshape(-1) - test_obs['E'].reshape(-1)).mean()\n",
    "f_mae = np.abs(obs_pred['F'].reshape(-1) - test_obs['F'].reshape(-1)).mean()\n",
    "print('energy mae: {} kcal/mol // force mae: {} kcal/mol A'.format(e_mae, f_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlff also provides a function for model evaluation where you use pre-implemented metric functions\n",
    "# or alternatively your own metric functions. For that, have a look at the source code if you want to implement\n",
    "# your own.\n",
    "\n",
    "from mlff.src.inference.evaluation import evaluate_model, mae_metric, rmse_metric\n",
    "\n",
    "metrics, obs_pred_2 = evaluate_model(params=test_params, \n",
    "                                     obs_fn=test_obs_fn,\n",
    "                                     data=test_data_tuple(d_test['test']),\n",
    "                                     metric_fn={'mae': mae_metric,\n",
    "                                                'rmse': rmse_metric},\n",
    "                                     batch_size=10)\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on Different Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets assume we have trained a model on the MD17 benchmark as above and want to apply if to a different dataset\n",
    "# e.g. QM7-X where the keys e.g. for atomic positions and atomic types are different from the ones in the MD17\n",
    "# dataset. In order to deal with that, one can re-set the property keys of the StackNet (and all its submodules).\n",
    "\n",
    "qm7x_prop_keys = {'energy': 'ePBE0+MBD',\n",
    "                  'force': 'totFOR',\n",
    "                  'atomic_position': 'atXYZ',\n",
    "                  'atomic_type': 'atNUM'}\n",
    "\n",
    "qm7x_E_key = qm7x_prop_keys['energy']\n",
    "qm7x_F_key = qm7x_prop_keys['force']\n",
    "qm7x_R_key = qm7x_prop_keys['atomic_position']\n",
    "qm7x_z_key = qm7x_prop_keys['atomic_type']\n",
    "\n",
    "qm7x_data = dict(np.load('example_data/qm7x_226.npz'))\n",
    "qm7x_dataset = DataSet(prop_keys=qm7x_prop_keys, data=qm7x_data)\n",
    "qm7x_data_split = qm7x_dataset.random_split(n_train=10,\n",
    "                                            n_valid=10,\n",
    "                                            n_test=40,\n",
    "                                            r_cut=5,\n",
    "                                            training=False,\n",
    "                                            seed=0)\n",
    "data_tuple = DataTuple(input_keys=[qm7x_R_key, qm7x_z_key, 'idx_i', 'idx_j'], target_keys=[qm7x_E_key, qm7x_F_key])\n",
    "test_input, test_obs = data_tuple(qm7x_data_split['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm7x_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we get an error, since stacknet expects a 'z' as key for the atomic numbers.\n",
    "test_obs_fn(test_params, test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now reset the property keys of the trained stack net, as well as for all its submodules.\n",
    "test_net.reset_prop_keys(qm7x_prop_keys, sub_modules=True)\n",
    "test_obs_fn_resetted = jax.vmap(jax.jit(get_obs_and_force_fn(test_net)), (None, 0))\n",
    "test_obs_fn_resetted(test_params, test_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlff-public",
   "language": "python",
   "name": "mlff-public"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
