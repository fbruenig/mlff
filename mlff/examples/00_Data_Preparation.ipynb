{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from mlff.src.data import DataSet\n",
    "\n",
    "# set the property keys that set the correspondence between the keys in the npz file and the chemical quantities\n",
    "prop_keys = {'energy': 'E',\n",
    "             'force': 'F',\n",
    "             'atomic_type': 'z',\n",
    "             'atomic_position': 'R',\n",
    "             }\n",
    "\n",
    "E_key = prop_keys['energy']\n",
    "F_key = prop_keys['force']\n",
    "R_key = prop_keys['atomic_position']\n",
    "z_key = prop_keys['atomic_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the npz data. As the DataSet object works with dictionarys we have to transform it to a dictionary first.\n",
    "data_path = 'example_data/ethanol.npz'\n",
    "data = dict(np.load(data_path))\n",
    "\n",
    "# Initialize a DataSet object with the property keys and the loaded (and dict transformed) data set.\n",
    "md17_dataset = DataSet(prop_keys=prop_keys, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A data set object supports three different split functions, which are `random_split`, `strat_split` and \n",
    "# `index_split` and all split the data into training, validation and testing data. The `random_split` function\n",
    "# randomly selectes `n_train`, `n_valid` and `n_test` data points. The `strat_split` function does select\n",
    "# data points to match the distribution of the quantity in `strat_key` as good as possible. The `index_split`\n",
    "# function splits the data given the data point indices data file.\n",
    "\n",
    "# If `n_test = None`, it takes all points\n",
    "# that are not part of the training and validation data as test points. If `r_cut` is not `None`, the function\n",
    "# also calculates the neighborhood lists for all geometries. The `training` argument allows to control if the \n",
    "# neighborhood lists should be also calculated for the test data set, since the test data is not required during\n",
    "# training. For large data sets, e.g. MD17, this saves the overhead of calculating neighborhood lists that are not\n",
    "# needed at training time. The `split_name` argument allows to give the split a custom name in order to save and load\n",
    "# it afterwards. If no `split_name` is passed, it defaults to `random_split`, `strat_split` and `index_split`, \n",
    "# respectively. If the same name is used twice, it is overwritten internally.\n",
    "\n",
    "\n",
    "random_split = md17_dataset.random_split(n_train=20, \n",
    "                                         n_valid=20, \n",
    "                                         n_test=None,\n",
    "                                         training=True,\n",
    "                                         r_cut=5., \n",
    "                                         seed=0,\n",
    "                                         split_name='my_random_split')\n",
    "\n",
    "\n",
    "strat_split = md17_dataset.strat_split(n_train=30, \n",
    "                                       n_valid=30, \n",
    "                                       n_test=10,\n",
    "                                       training=True,\n",
    "                                       r_cut=5., \n",
    "                                       seed=0,\n",
    "                                       strat_key=E_key,\n",
    "                                       split_name='my_strat_split')\n",
    "\n",
    "\n",
    "index_split = md17_dataset.index_split(data_idx_train=np.array([0, 1, 2]), \n",
    "                                       data_idx_valid=np.array([3, 4]), \n",
    "                                       data_idx_test=np.array([5], dtype=int), \n",
    "                                       r_cut=5.,\n",
    "                                       split_name='my_index_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# At the top level, we find the data indicees that give the index of the data point in the original data file.\n",
    "print(strat_split.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going e.g. one level deeper, we find that each quantity has its own entry under which we find the data.\n",
    "print(strat_split['train'].keys())\n",
    "# So the energies in the training set can be called as\n",
    "print('Energy data shape: {}'.format(strat_split['train']['E'].shape))\n",
    "print('Energy data:\\n {}'.format(strat_split['train']['E']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save a data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we look at the splits value of the data set object, we find index lists for each split. In order to recover\n",
    "# splits, e.g. for testing, one can save the splits to a file using the `save_splits_to_file` function.\n",
    " \n",
    "pprint(md17_dataset.splits)\n",
    "md17_dataset.save_splits_to_file(path='', filename='my_splits.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The saved index lists can be loaded using the `load_splits_from_file` function.\n",
    "\n",
    "my_saved_splits = md17_dataset.load_splits_from_file(path='', filename='my_splits.json')\n",
    "list(my_saved_splits.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If one is interested in the data itself, rather than only in the index lists, one can load the data set \n",
    "# by using the `load_split` function.\n",
    "rec_strat_split = md17_dataset.load_split(file='my_splits.json',\n",
    "                                          r_cut=5.,\n",
    "                                          split_name='my_strat_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One can additionally pass n_train, n_valid and n_test arguments to the function, if one only wants to\n",
    "# recover e.g. a subpart of the data. `None` defaults \n",
    "rec_random_split = md17_dataset.load_split(file='my_splits.json',\n",
    "                                           r_cut=5.,\n",
    "                                           n_train=None,\n",
    "                                           n_valid=None,\n",
    "                                           n_test=100,\n",
    "                                           split_name='my_random_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the recovered splits actually match the original ones\n",
    "\n",
    "rec_random_split['train'][R_key].all() == random_split['train'][R_key].all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlff-public",
   "language": "python",
   "name": "mlff-public"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
